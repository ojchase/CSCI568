<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<title>Portfolio 3 - Cluster Analysis</title>
		<meta name="author" content="Oliver"/>
	</head>
	<body>
		<h1>Cluster Analysis</h1>
		<div id="Content">
			<div id="Intro">
				<h2>Why Do We Care?</h2>
				<p>Cluster analysis allows us to find groups ("clusters") of related objects.  This may be useful in, for example, marketing, as you may want to create three different advertisements that would attract the most possible customers, without just attracting the same people multiple ways.</p>
				<p>Let's say you're trying to market a  productivity app.  You would apply cluster analysis to analyze your existing customer base and might discover that your ads will be the most effective if you target single moms, businessman dads, and overworked party animals.  This third category may not have been previously obvious, and it could indicate a underutilized market just waiting for your app.  Occasionally they find it on their own, and were thus found by the clustering algorithm, but think how many more you could get with some targeted advertising!</p>
			</div> <!--end of intro-->
			<div id="Algorithms">
				<h2>How Do We Do It?</h2>
				<p>First, please excuse the low-tech graphics.  They're extremely simple and contrived to make a point, and made with Paint to avoid having to deal with more complex programs or getting copyright permission from others.</p>
				<p>There are multiple ways to cluster data, and none is perfect.  Perhaps the best one is still to be discovered, or is dependent on your specific use.  Here's a few, or you could always create your own!</p>
				<div id="KMeans">
					<h3>K-Means</h3>
					<p>K-Means is a prototype based clustering algorithm, which means it uses a representative data point ("prototype") to generate the clusters.  The beauty of K-Means is that this prototype moves around until reasonably optimal.  At the end you can describe the clusters reasonably well just by looking at the prototype points.  We'll use this dataset to illustrate.  Note the boundary where the reasonable human would mentally place the two clusters and that I colored the two clusters differently:</p>
					<img src="kMeans/step0.png" alt="Initial Data for K-Means">
					<p>First, you'll want to decide how many clusters you want to achieve.  This may be obvious from your application, but it could also be unknown, so make a reasonable guess for now.  We'll call this number K.  Next, you'll want to randomly generate K points ("centroids") somewhere throughout your data.  The initial centroids should be random, but they should also be reasonable.  If all your data is between 0 and 3 in each dimension, it won't make sense to put a centroid at <3, 458, -34>, so limit it to the bounds of the data.  Here we've removed the mental cluster boundaries, reset the points to unclustered black, and added two random, separately-colored centroids:</p>
					<img src="kMeans/step1.png" alt="Added red and blue centroids">					
					<p>Now the fun begins.  You have K initial centroids, and each data point is going to find the closest centroid to itself.  How, you may ask?  Well, it should use some similarity metric between itself and each centroid, selecting the centroid with the highest similarity.  (Euclidean Distance will probably work well for such a problem.)  Now each point has identified itself with the closest centroid, and we've color-coded it to help you visualize:</p>
					<img src="kMeans/step2.png" alt="Points assigned centroids">
					<p>Next is the most important step: The centroids now are forced to move to become more representative of their following points.  In other words, we average all of the data points belonging to each cluster and move the centroid there:</p>
					<img src="kMeans/step3.png" alt="Centroids have moved">
					<p>Now we just repeat the last two steps until we reach a stopping condition.  The points will relocate themselves to whichever centroid is nearest, and the centroids will move to their clusters' average position, and so on.</p>
					<img src="kMeans/step4.png" alt="Points have changed centroids">
					<p>When do we stop?  This may depend on how complex your dataset is, but the easiest reasonable stopping condition is when you go through a full iteration with no point changing its centroid selection.  Another might be to stop when the SSE (a measure of the tightness of the clusters) fails to improve, but that's a little outside the scope of this page.  Here, the centroids are now about where you'd expect and the points are all clustered correctly.  You can continue running the algorithm all you want, but no point is ever going to switch clusters, so it's effectively done.</p>
					<img src="kMeans/step5.png" alt="Centroids have moved, and points will not change clusters again">
					<p>In summary, K-Means is a relatively simple algorithm to implement (see below, I actually did implement this one) but does have some issues to address.  For one thing, you need to decide how many clusters to use.  One approach is to use the aforementioned, but intentionally not described, SSE to decide the correct number.  You just run the algorithm multiple times with multiple K and at some point the SSE will begin to behave differently than before if you plot it against K.  (A "knee" in the graph.)  It's effectively a big optimization problem, allowing you to find an ideal K.  We also need to recognize that K-Means will not do well with outliers (which may tug at the centroids excessively) or non-globular shapes.  Give it two intersecting lines to work with, and you will not be happy with the results!</p>
					<pre>
class KMeans
  #returns the cumulative SSE from adding each of them together
  def run(objects, k, print)
    centroids = initCentroids(objects, k)
    #print(centroids)
    nochanges = false

    while(!nochanges)
      nochanges = true
      objects.each do |obj|
        moved = pickNearestCentroid(obj, centroids)
        if(moved)
          nochanges = false
        end
      end

      recalculateCentroids(objects, centroids)

    end

    cumulativeSSE = showResults(objects, centroids, print)
    return cumulativeSSE
  end

  #returns the from all the clusters sse
  def showResults(objects, centroids, print)
    sse = 0
    if (print)
      centroids.each do |centroid|
        puts centroid
        puts "Includes the following points:"
        objects.each do |obj|
          if(obj.getCentroid == centroid)
            puts "    #{obj}"
          end
        end
        puts("SSE: #{sse(objects, centroid)}")
        puts
        sse = sse + sse(objects, centroid)
      end
    else
      centroids.each do |centroid|
        sse = sse + sse(objects, centroid)
      end
    end
    return sse
  end

  def sse(objects, centroid)
    sse = 0
    objects.each do |obj|
      if(obj.getCentroid == centroid)
        sse = sse + (euclideanNotNormalized(obj, centroid.getDataPoint))**2
      end
    end
    return sse
  end

  def recalculateCentroids(objects, centroids)
    len = objects[0].values.length
    centroids.each do |centroid|
      dimensionSums = Array.new(len)
      dimensionSums.fill(0)
      usingCentroid = 0
      #puts "Centroid #{centroid.getID} started at #{centroid.getCoordinates}"
      objects.each do |obj|
        if(obj.getCentroid == centroid)
          #puts "   #{obj} is on this centroid!"
          for i in (0..len-1)
            dimensionSums[i] = dimensionSums[i] + obj.values[i]
          end
          usingCentroid = usingCentroid + 1
        end
      end
      #print dimensionSums
      for i in (0..dimensionSums.length - 1)
        #puts dimensionSums[i]
        #puts usingCentroid
        dimensionSums[i] = dimensionSums[i] / usingCentroid.to_f
      end
      if(usingCentroid == 0) #we'll just move the centroid to a random point - better than having an empty cluster!
        dimensionSums = objects[rand(objects.length)].values
      end
      centroid.setCoordinates(dimensionSums)
      #puts "Centroid #{centroid.getID} moved to #{centroid.getCoordinates}"
    end
  end

  #moves object to the closest centroid and returns true if it moved
  def pickNearestCentroid(object, centroids)
    #puts "#{object} is currently on centroid #{object.getCentroid}"
    currentCentroid = object.getCentroid()

    len = centroids.length
    distanceToCentroids = Array.new(len)

    for i in (0..len-1)
      #puts "    #{object} is #{1 - euclidean(object, centroids[i].getDataPoint)} from #{centroids[i]}"
      distanceToCentroids[i] = 1 - euclidean(object, centroids[i].getDataPoint)
    end
    closestCentroid = centroids[distanceToCentroids.index(distanceToCentroids.min)]
    object.setCentroid(closestCentroid)
    #puts "#{object} is now on centroid #{object.getCentroid}"
    return currentCentroid.nil? || currentCentroid != closestCentroid
  end

  def initCentroids(objects, k)
    len = objects[0].values.length
    objects.each do |obj|
      if(len != obj.values.length)
        return "Clustering can't continue as objects have a different number of attributes!"
      end
    end
    minBounds = Array.new(len)
    maxBounds = Array.new(len)

    #Calculate the minimum and maximum allowable value for each attribute
    objects.each do |obj|
      i = 0
      obj.values.each do |value|
        if(minBounds[i].nil? || minBounds[i] > value)
          minBounds[i] = value
        end
        if(maxBounds[i].nil? || maxBounds[i] < value)
          maxBounds[i] = value
        end
        i = i + 1
      end
    end

    #Randomly pick centroids in that range!
    centroids = Array.new(k)
    r = Random.new
    for i in (0..k-1)
      center = Array.new(len)
      for j in (0..len-1)
        center[j] = rand(maxBounds[j] - minBounds[j]) + minBounds[j]
      end
      centroids[i] = Centroid.new(i, center)
    end
    return centroids
  end

end
					</pre>
				</div> <!-- end of KMeans-->
				<div id="AgglomerativeHierarchical">
					<h3>Agglomerative Hierarchical</h3>
					<p>Agglomerative hierarchical clustering (let's abbreviate it as AHC) works very differently; in fact, it won't return more than one cluster at all, but just one that has subclusters within it.  Effectively it's one cluster that is an agglomeration of smaller clusters, each of which will hopefully be meaningful at its level of resolution.  But if you want to generate some number of clusters other than one, it'll take some work to break the result down.  Let's use the same example as before to step through AHC.</p>
					<img src="AHC/step0.png" alt="Initial AHC data setup">
					<p>First, let's define every single point as a cluster.  The key to AHC is that you continually combine the clusters that are closest into one new cluster that contains both of the old clusters inside it.  Again, "closeness" depends on a similarity metric of your choosing and can be adjusted, but here we'll just use Euclidean Distance between the centers of the clusters.  So we merge the two closest centroids and then  just repeat the process until you're left with one cluster at the end.  So here, let's say these two points are the closest, so we combine them into a new cluster.  Note the red point indicating the new center of the cluster which we'll be comparing against.</p>
					<img src="AHC/step1.png" alt="Two closest points combined into a new cluster">
					<p>Now maybe these two points on the other side are the next closest cluster; let's combine them into one.</p>
					<img src="AHC/step2.png" alt="Two closest points combined into a new cluster">
					<p>At this point, we have five clusters - 2 of size 2 and 3 of size 1.  The size 1 clusters do not necessarily have to be combined with each other first, although that can happen and did for the first two steps.  In this case, the two clusters on the left are now the closest, so we combine them.</p>
					<img src="AHC/step3.png" alt="One cluster and one point (itself a cluster) combined into a new cluster">
					<p>Hopefully you get the idea.  Now we merge two clusters on the right which are relatively close.</p>
					<img src="AHC/step4.png" alt="Two more clusters merged">
					<p>And the right supercluster adds the lone straggler on the right side.</p>
					<img src="AHC/step5.png" alt="Two more clusters merged">
					<p>We only have two clusters left, so they get combined and we now have one large cluster!</p>
					<img src="AHC/step6.png" alt="Two more clusters merged - now we just have one big one!">
					<p>The biggest issue with AHC is that it can be very computationally intensive.  At the beginning you have to calculate the distance from every point to every other point, which can take a long time with a large, high-dimensional dataset.  And then with each merge you'll have to calculate the distance from the new center to every other center all over again.</p>
				</div> <!-- end of AgglomerativeHierarchical-->
				<div id="DBSCAN">
					<h3>DBSCAN</h3>
					<p>Our class had trouble getting the idea behind DBSCAN, which presumably stands for Density-Based Scan, but the idea is actually quite simple.  DBSCAN looks for areas of some density by finding "core" points which have the required number of points around them within the given range.  For a real life example, if you want a density of 3 people per square mile, you could look at every person in the country and draw a circle outward with an area of a square mile.  Any person that encounters three people around him in that region is in an area of at least that density.  We call these people (or points) "core" points, and they define a cluster, along with the people they encountered.  If any of those people are themselves core points, we include their close connections, and so grow the cluster out.  Any connections that are around a core point but do not have enough close connections of their own to meet the density requirement are called "border" points.  We don't care about any other points and throw them out as noise.</p>
					<p>Again, the exact algorithm you use to get to the result is debateable and can be optimized, but the end result is that every point is classified as either a core point, border point, or noise.  The core points have some number of connections within some distance of themselves, and you get to set those parameters.  The border points connect to a core point, but are not themselves connected enough to be core points.  The others are noise and are discarded.</p>
					<p>DBSCAN has two main advantages.  One is that it can tolerate and even eliminate noise.  It's built right into the algorithm!  The other is that it can find non-globular regions, as the core points can be in any shape, provided that they can connect to each other.</p>
				</div> <!-- end of DBSCAN-->
			</div> <!-- end of algorithms-->
		</div> <!--end of content-->
		<div class="MoreInfo">
			<h2>More Information</h2>
			<p>See chapter 8 of <em>Introduction to Data Mining</em> by Tan, Steinbach, and Kumar, 2006.</p>
		</div> <!--end of MoreInfo-->
		<div class="Copyright">
			<p>&copy; 2011 by Oliver Chase. All rights reserved.</p>
		</div>
	</body>
</html>