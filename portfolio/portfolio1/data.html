<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<title>Portfolio 1 - Data & Data Quality</title>
		<meta name="author" content="Oliver"/>
	</head>
	<body>
		<h1>Data & Data Quality</h1>
		<div id="Content">
			<div id="Intro">
				<h2>Why Do We Care?</h2>
				<p>You're joking, right?  Data mining uses, well, data, and all the time.  It's kind of the point.  So we need to have some idea of what it is and what to be careful of.  We'll cover some of the different kinds of data and what issues they present.  Further, all the best data mining algorithms in the world will be utterly useless to us if the data we use is not good quality, so we'll look at data quality issues and how to resolve them.</p>
			</div> <!--end of intro-->
			<div id="DatasetTypes">
				<h2>Dataset Types</h2>
				<ul>
					<li>
						<h3>Record data</h3>
						<p>Record data is basically a list of objects, each with one or more attributes.  For example, we did a survey for CSCI522 and our records were response sets, one per person.  Market basket data fits here as well, where each customer is a record and all the possible items are the attributes, most of which will be 0 but a few (important ones!) will be nonzero.</p>
					</li>
					<li>
						<h3>Graph-based data</h3>
						<p>Graph-based data is used to express relationships.  You might display friend networks or web links using some sort of graph dataset.</p>
					</li>
					<li>
						<h3>Ordered data</h3>
						<p>These are datasets where there is some ordering between the values.  For example, you may be recording your mood at various times of the day (time series data) or the population densities across the planet (spatial data), or even worse both at the same time (population over time)!  Simple sequences of data or events would go here too.</p>
					</li>
				</ul>
				<p>Note: We didn't talk much about graph or ordered data.  The algorithms we used were designed for record data but could be extended for the other types.  Going forward, assume we're just using record data.</p>
				<p>Other concepts: You also want to pay attention to the dimensionalty and sparsity of the data in the dataset.  Dimensionality refers to the number of different attributes in the dataset.  Too many different attributes (i.e. high dimensionality) can cause some algorithms to work slowly.  Also, you may be able to work more efficiently when you recognize your dataset is sparse, which means it mostly has "0"s in the data, as with market basket data.  Often, all you really have to pay attention to is the presence of values, and a 0 can be ignored, reducing the dataset size signficantly.</p>
			</div> <!--end of DatasetTypes-->
			<div id="AttributeTypes">
				<h2>Attribute Types</h2>
				<p>The first thing you have to be aware of is the types of data you're using.  I discovered in another project that binary data was more useful to me than a range of integer values, and so it took a good amount of time to binarize my data into a useful format.  Most data types fit one of two categories: Numeric or Categorical.</p>
				<ul>
					<li>
						<h3>Numeric</h3>
						<p>These are pretty intuitive: any form of data that has a number, often continuous, attached.  You may have heard of them in science education as "quantitative" data.  We can further narrow this down into interval or ratio data:
						<ul>
							<li>
								<h4>Interval</h4>
								<p>In my opinion, these are the most obvious data to understand, and hardly need description.  Interval data is anything where you can put different numbers and the difference between them is meaningful.  For example, seven cats is exactly six more than one cat.  (Actually, this is ratio data, but hold that thought.)  Another example is dates; October 27 is an interval value, as is October 3.  The gap of 24 days is meaningful.</p>
							</li>
							<li>
								<h4>Ratio</h4>
								<p>For ratio data, not only is the gap between values meaningful, but the ratio between them should be too.  October 27 is not nine times larger than October 3 under normal circumstances, so it's not a ratio.  However, seven cats is not ony six more than one cat (and thus qualifies as interval data), but it's also more specifically seven times larger than one cat, so it's also the more restrictive ratio data. </p>
							</li>
						</ul>
						<p>Another example distinguishing interval and ratio data is temperature.  Ask yourself: Is 80 degrees Fahrenheit twice as warm as 40 degrees Fahrenheit?  I can't really answer that on a whim, but the science would indicate "no".  Thus, it's merely an interval.  Kelvin, however, is different.  As it's based on absolute zero and not arbitrary temperatures, you can say that 200 Kelvin is half of 400 Kelvin.  Thus it's ratio data.</p>
					</li>
					<li>
						<h3>Categorical</h3>
						<p>Categorical data look obvious at first, but can be easily mixed up with the numeric data if you're not careful.  Why?  Because they often look like numbers, and will probably be represented in your data set with integers.  But there's an important subtle difference.  Categorical, or "qualitative", data represent distinct (<em>discrete</em>) data elements, and can be nominal or ordinal.</p>
						<ul>
							<li>
								<h4>Nominal</h4>
								<p>I find nominal data to be the easiest, after interval numeric data above, to grasp intuitively.  These are distinct elements that don't have any particular ordering to them.  For example, if you're a bartering service, the available products would probably be nominal attributes: bookshelf, coffee maker, Newsboys CD, and so on.  The color of the item would itself be nominal (e.g. red or blue).</p>
								<p>Binary data would go here.</p>
							</li>
							<li>
								<h4>Ordinal</h4>
								<p>Ordinal data are superficially similar to nominal data, but they also possess some sort of inherent ordering.  For example, the grade levels "A", "B", "C", "D", and "F" are distinct elements, but there is also a definite order, and the "B" better come before "C".</p>
							</li>
						</ul>
					</li>
				</ul>
				<p>Notice too that these categories are just a little bit fuzzy.  For example, "red" and "blue" were classified above as nominal values, but they could just as well be represented by their wavelengths, which are actually ratio numbers (e.g. about 475nm for blue and 650nm for red).  "Bob" and "Sue" are also nominal, but if you provide them with distinct employee IDs, then they can become sortable as ordinal data.  (You could argue that even then they're still nominal, but those IDs may be meaningful and store extra information, like the order of hire, that would make their order matter.)</p>
				<p>Why does this matter?  Some of the distinctions are related to the concept of <em>transformations</em>.  You may at some point find that your column of data from -48.2847 to 55 is not particularly convenient to work with and wish to change it to a 0 to 100 scale.  How you're allowed to do that will depend on the data type.  If they're nominal values, you would wonder why you ever had a -48.2847 in the first place, but you can arbitrarily change it to anything else if you wish, as long as it doesn't conflict with another value.  But if it indicates a set interval between itself and other values, then you better adjust all the values by the same amount (i.e. now your scale is 0 to 103.2847).</p>
				<p>Also, we did a project with mushroom data, trying to predict which are poisonous and which aren't, which used a lot of nominal data, such as the type of ring on the mushroom (i.e. bulbous, club, cup, equal, rhizomorphs, or rooted).  You have to be aware that these are nominal attributes, because you really don't want to say that bulbous and club ring types are only one different from each other and therefore very similar mushrooms.  I don't know what these terms actually mean, but I'm not willing to stake my life on the dissimilarity of a cup or rooted ring, are you?</p>
			</div> <!-- end of AttributeTypes-->
			<div id="DataQuality">
				<h2>Data Quality</h2>
				<p>As mentioned, no matter how wonderful the data mining could eventually be, if your data itself is not in good shape, you're not going to get anything meaningful.  There are a number of potential data quality issues.  One is that there were errors in the collection process.  Maybe the thermometer was calibrated incorrectly and every value is really a couple degrees higher.  Regardless, there will still always be unexpected values in your data, which we refer to as noise.  This is a result of natural variation in real-world measurements; no matter how good your technique is, nature just won't cooperate sometimes.  Some of the mining algorithms we'll cover can ignore noise quite effectively; some can't.</p>
				<p>Of course some noise is not noise at all, but an identity theft that really does leave a different transaction imprint.  This is referred to as an outlier, and we actually do want to find them.  Sometimes outliers are quite interesting; just ask the person losing his identity!</p>
				<p>The next problem you may run into is missing values.  This is particularly bad when the data is generated by humans instead of an automated process.  Human perception is a messy thing after all.  <em>Beautiful Data</em> talks about this some, and our survey again worked through it as well.  It was very tricky to tell if a missing value meant the user consciously chose not to check a box, or had just failed to answer the question at all.  It would always be better to fix the data collection process, but sometimes it's just not practical.  (It's not like we're resending the survey worded better!)  There's a few things we can do with missing values.  We can elminate the entire object, but then you might lose other interesting data.  Better is to ignore just the missing values or to estimate them, using an average of everyone's value or maybe the cluster the person best fits in (see section 3).  In our case, we had to manually convert qualitative answers into binary categories, delete some records entirely, and generally just ignore missing values.</p>
				<p>Also, you'll probably need to do some preprocessing (data modification before analysis) to remove bad records, reformat your data, and so on.  For example, you may want to separate your nominal attributes into multiple binary attributes, one per possible nominal value, which will work better later, a process called binarization.  In theory, good preprocessing should be used to resolve data quality issues before running algorithms.</p>
			</div> <!--end of DataQuality-->
			<div id="ExploringData">
				<h2>Exploring Data</h2>
				<p>The first step of the data mining process is to explore your data, just so you have some idea of what is inside it.  This isn't particularly difficult; just scan a sample (preferably random) of your data, and maybe run it through some tool to get some summary statistics, like these for the Iris dataset:</p>
				<img src="Weka_Summary_Stats_for_Iris_Dataset.png" alt="Summary statistics on the Iris dataset, generated by Weka">
				<img src="Knime_Summary_Stats_for_Iris_Dataset.png" alt="Summary statistics on the Iris dataset, generated by Knime">
			</div> <!--end of ExploringData-->
			<div id="DataVisualization">
				<h2>Data Visualization</h2>
				<p>The next step of the process is to visualize the data in some way.  This can be tricky with high dimensionality, but can be done.  Again, the purpose is just to help you personally get your ahead around the data so you have an initial direction to go in your mining.  It may also help you communicate it better later. </p>
				<p>We used a tool called Processing for visualization in this class.  It's pretty easy to use and quite extensible.  Here's an example visualization of some "random" numbers, where the brighter a number is, the more it was chosen:
				<img src="processing.png" alt="Processing visualization of some supposedly random numbers">
				<p>As you can see, some of the numbers (e.g. 17, 42 [odd huh?], 77) are a lot more common than others, which you would not expect from a random dataset.  Indeed, these were actually generated by humans, and the issues with the data's integrity, properly visualized, now stick out like a sore thumb.  This is the point of visualization: to help your brain interpret all the data and find patterns on its own.  The human brain is better than any algorithm if given a good visualization, so this is an important second step.</p>
			</div> <!--end of DataVisualization-->
		</div> <!--end of content-->
		<div class="MoreInfo">
			<h2>More Information</h2>
			<p>See chapter 2 of <em>Introduction to Data Mining</em> by Tan, Steinbach, and Kumar, 2006. <em>Beautiful Data</em> (by Segaran and Hammerbacher, 2009) also discusses data quality issues throughout the book.  The survey mentioned is in chapter 2.</p>
		</div> <!--end of MoreInfo-->
		<div class="Copyright">
			<p>&copy; 2011 by Oliver Chase. All rights reserved.</p>
		</div>
	</body>
</html>